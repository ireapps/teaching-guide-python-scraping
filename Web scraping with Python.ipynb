{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with Python\n",
    "\n",
    "This notebook demonstrates how you can use the Python programming language to scrape information from a web page.\n",
    "\n",
    "The goal today: Scrape the main table on [the first page of FDA warning letters issued in 2019](https://www.fda.gov/ICECI/EnforcementActions/WarningLetters/2019/default.htm) and, if time, write the data to a CSV.\n",
    "\n",
    "If you're relatively new to Python, it might be helpful to have [this Python syntax cheat sheet](Python%20syntax%20cheat%20sheet.ipynb) open in another tab as you work through this notebook.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- [Using Jupyter notebooks](#Using-Jupyter-notebooks)\n",
    "- [What _is_ a web page, anyway?](#What-is-a-web-page,-anyway?)\n",
    "- [Inspect the source](#Inspect-the-source)\n",
    "- [Import libraries](#Import-libraries)\n",
    "- [Request the page](#Request-the-page)\n",
    "- [Turn your HTML into soup](#Turn-your-HTML-into-soup)\n",
    "- [Targeting and extracting data](#Targeting-and-extracting-data)\n",
    "- [Write the results to file](#Write-the-results-to-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jupyter notebooks\n",
    "\n",
    "There are several ways to write and run Python code on your computer. One way -- the method we're using today -- is to use [Jupyter notebooks](https://jupyter.org/), which run in your browser and allow you to intersperse documentation with your code. They're handy for bundling your code with a human-readable explanation of what's happening at each step. Check out some examples from the [L.A. Times](https://github.com/datadesk/notebooks) and [BuzzFeed News](https://github.com/BuzzFeedNews/everything#data-and-analyses).\n",
    "\n",
    "**To add a new cell to your notebook**: Click the + button in the menu.\n",
    "\n",
    "**To run a cell of code**: Select the cell and click the \"Run\" button in the menu, or you can press Shift+Enter.\n",
    "\n",
    "**One common gotcha**: The notebook doesn't \"know\" about code you've written until you've _run_ the cell containing it. For example, if you define a variable called `my_name` in one cell, and later, when you try to access that variable in another cell but get an error that says `NameError: name 'my_name' is not defined`, the most likely solution is to run (or re-run) the cell in which you defined `my_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What _is_ a web page, anyway?\n",
    "\n",
    "Basically, a collection of specifically formatted text files stored on a computer (a _server_) that's probably sitting on a rack in a giant data center somewhere.\n",
    "\n",
    "Mostly you'll be dealing with `.html` (HyperText Markup Language) files that typically include references to `.css` (Cascading Style Sheet) files, which determine how the page looks, and/or `.js` (JavaScript) files, which add interactivity.\n",
    "\n",
    "Today, we'll focus on the HTML, which provide structure to the page.\n",
    "\n",
    "HTML elements are represented by a pair of tags -- an opening tag and a closing tag.\n",
    "\n",
    "A table, for example, starts with `<table>` and ends with `</table>`. The first tag tells the browser: \"Hey! I got a table here! Render it as a table.\" The closing tag (note the forward slash!) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested more HTML tags representing rows (`<tr>`) and cells (`<td>`).\n",
    "\n",
    "HTML elements can have any number of attributes, such as classes --\n",
    "\n",
    "`<table class=\"cool-table\">`\n",
    "\n",
    "-- styles --\n",
    "\n",
    "`<table style=\"width:95%;\">`\n",
    "\n",
    "-- hyperlinks to other pages --\n",
    "\n",
    "`<a href=\"https://ire.org\">Click here to visit IRE's website</a>`\n",
    "\n",
    "-- and IDs --\n",
    "\n",
    "`<table id=\"cool-table\">`\n",
    "\n",
    "-- that will be useful to know about when we're scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the source\n",
    "\n",
    "You can look at the HTML that makes up a web page by _inspecting the source_ in a web browser. We like Chrome and Firefox for this; today, we'll use Chrome.\n",
    "\n",
    "To \"view source\" in Chrome, hit `Ctrl+U` on a PC or `‚åò+Opt+U` on a Mac. (It's also in the menu bar: View > Developer > View Page Source.)\n",
    "\n",
    "You'll get a page showing you all of the HTML code that makes up that page. Ignore 99% of it and try to locate the element(s) that you want to target (use `Ctrl+F` on a PC and `‚åò+F` to find).\n",
    "\n",
    "You can also inspect specific elements on the page by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\" from the context menu that pops up. Hover over elements in the \"Elements\" tab to highlight them on the page.\n",
    "\n",
    "Open up a Chrome browser and inspect the table on the [first page of FDA's list of warning letters issued in 2018](https://www.fda.gov/ICECI/EnforcementActions/WarningLetters/2019/default.htm). Find the table we want to scrape.\n",
    "\n",
    "Is it the only table on the page? If not, does it have any attributes that would allow you to target it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Step one is to _import_ two third-party Python libraries that will help us scrape this page:\n",
    "- `requests` is the de facto standard for making HTTP requests, similar to what happens when you type a URL into a browser window and hit enter.\n",
    "- `bs4`, or BeautifulSoup, is a popular library for parsing HTML into a data structure that Python can work with. In this case, though, we only need the `BeautifulSoup` object that comes with `bs4`, so we'll only import that. (Bonus: That's the convention you'll see when Googling for help.)\n",
    "\n",
    "These libraries are installed separately from Python on a per-project basis ([read more about our recommendations for setting up Python projects here](https://docs.google.com/document/d/1cYmpfZEZ8r-09Q6Go917cKVcQk_d0P61gm0q8DAdIdg/edit#heading=h.od2v1nkge5t1)).\n",
    "\n",
    "Run this cell (you'll only have to do this once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request the page\n",
    "\n",
    "Next, we'll use the `get()` method of the `requests` library (which we just imported) to grab the web page.\n",
    "\n",
    "While we're at it, we'll _assign_ all the stuff that comes back to a new variable using `=`.\n",
    "\n",
    "The variable name is arbitrary, but it's usually good to pick something that describes whatever value it's pointing to.\n",
    "\n",
    "Notice that the URL we're grabbing is wrapped in quotes, making it a _string_ that Python will interepret as text (as opposed to numbers, booleans, etc.). You can read up more on Python data types and variable assignment [here](Python%20syntax%20cheat%20sheet.ipynb).\n",
    "\n",
    "Run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_page = requests.get('https://www.fda.gov/ICECI/EnforcementActions/WarningLetters/2019/default.htm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing appears to have happened, which is (usually) a good sign.\n",
    "\n",
    "If you want to make sure that your request was successful, you can check the `status_code` attribute of the Python object that was returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `200` code means all is well. `404` means the page wasn't found, etc. ([Here's one of our favorite lists of HTTP status codes](https://http.cat/) ([or here, if you prefer dogs](https://httpstatusdogs.com/)).)\n",
    "\n",
    "The object being stored as the `fda_page` variable came back with a lot of potentially useful information we could access. Today, we're mostly interested in the `.text` attribute -- the HTML that makes up the web page, same as if we'd viewed the page source. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda_page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Try it yourself\n",
    "\n",
    "Use the code blocks below to experiment with requesting web pages and checking out the HTML that gets returned.\n",
    "\n",
    "Some ideas to get you started:\n",
    "- `'http://ire.org'`\n",
    "- `'https://web.archive.org/web/20031202214318/http://www.tdcj.state.tx.us:80/stat/finalmeals.htm'`\n",
    "- `'https://www.nrc.gov/reactors/operating/list-power-reactor-units.html'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn your HTML into soup\n",
    "\n",
    "The HTML in the `.text` attribute of the request object is just a string -- a big ol' chunk of text.\n",
    "\n",
    "Before we start targeting and extracting pieces of data in the HTML, we need to turn that chunk of text into a data structure that Python can work with. That's where the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (`bs4`) library comes in.\n",
    "\n",
    "We'll create a new instance of the `BeautifulSoup` object we imported earlier, and we need to give it two things:\n",
    "- The HTML we'd like to parse -- `fda_page.text`\n",
    "- A string with the name of the type of parser to use -- `html.parser` is the default and usually fine, but [there are other options](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)\n",
    "\n",
    "We'll save the parsed HTML as a new variable, `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(fda_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happened, which is good! You can take a look at what `soup` is, but it looks pretty much like `fda_page.text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be sure, you can use the Python function `type()` to check what sort of object you're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `str` type means a string, or text\n",
    "type(fda_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `bs4.BeautifulSoup` type means we successfully created the object\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Try it yourself\n",
    "\n",
    "Use the code blocks below to experiment fetching HTML and turning it into soup (if you fetched some pages earlier and saved them as variables, that'd be a good start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeting and extracting data\n",
    "\n",
    "Now that we have BeautifulSoup object loaded up, we can go hunting for the specific HTML elements that contain the data we need. Our general strategy:\n",
    "1. Find the main table with the data we want to grab\n",
    "2. Get a list of rows (the `tr` element, which stands for \"table row\") in that table\n",
    "3. Use a Python `for loop` to go through each table row and find the data inside it (`td`, or \"table data\")\n",
    "\n",
    "To accomplish this, we'll use two `bs4` methods:\n",
    "- [`find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find), which returns the first element that matches whatever criteria you hand it\n",
    "- [`find_all()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all), which returns a _list_ of elements that match the criteria. ([Here's how Python lists work](Python%20syntax%20cheat%20sheet.ipynb#Lists).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the table\n",
    "\n",
    "To start with, we need to find the table. There are several ways to accomplish this, but because this is the only table on the page (view source and `Ctrl+F` to search for `<table` to confirm), we can simply say, \"Look through the `soup` object and find the table tag.\"\n",
    "\n",
    "Translated, the code is: `soup.find('table')`. While we're at it, save the results of that search to a new variable, `table`.\n",
    "\n",
    "Run these cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the rows in the table\n",
    "\n",
    "Next, use the `find_all()` method to drill down and get a list of rows in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many items are in this list -- in other words, how many rows are in the table -- you can use the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through the rows and extract the data\n",
    "\n",
    "Next, we can use a [`for` loop](Python%20syntax%20cheat%20sheet.ipynb#for-loops) to go through the list of rows and start grabbing data from each one.\n",
    "\n",
    "Quick refresher on _for loop_ syntax: Start with the word `for` (lowercase), then a variable name to stand in for each item in the list that you're looping over, then the word `in` (lowercase), then the name of the list holding the items (`rows`, in our case), then a colon, then an indented block of code describing what we're doing to each item in the list.\n",
    "\n",
    "Each piece of data in the row will be stored in a `td` tag, which stands for \"table data.\" So inside the loop -- in the indented block -- we'll use the `find_all()` method to get a list of every `td` tag inside the row. And from there, we can access the content inside each tag.\n",
    "\n",
    "Our goal is to end up with a _list_ of data for each row that we will eventually write out to a file. Typically you'd probably do the work of looping and inspecting the results, step by step, in one code cell. But to show the thinking of how you might approach this (and to practice the syntax), we'll start by just printing out each row and then build from there. (`print()` an empty line, too, to help you see what you're working with.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    print(row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first item that prints is the header row with the column labels, and that it contains `th` (\"table header\") tags instead of `td` (\"table data\") tags.\n",
    "\n",
    "Because our next step in our `for` loop was going to be, \"Find all of the `td` tags in this row,\" this is something we need to deal with. We could just tell the script \"Find `td` _or_ `th` tags,\" or we could just skip the first row altogether. I usually just skip the first row using _list slicing_: adding square brackets with some instructions about which items in the list you want to select.\n",
    "\n",
    "Here, the syntax would be: `rows[1:]`, which means, take everything in the `rows` list from the item in position 1 to then end. Like many programming languages, Python starts counting at 0, so the result will leave off the first item in the list -- i.e. the item in position 0, i.e. the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    print(row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're cooking with gas. Let's start pulling out the data in each row. Start by using `find_all()` to grab a lit of `td` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    print(cells)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have, for each row, a _list_ of `td` tags. Next step would be to look at the table and start grabbing specific values based on their position in the list and assigning them to human-readable variable names.\n",
    "\n",
    "Quick refresher on list syntax: To access a specific item in a list, use square brackets `[]` and the index number of the item you'd like to access. For instance, to get the first item in the cell, use `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    date = cells[0]\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is returning the entire Tag object -- we just want the contents inside it. You can access the `.string` attribute of the tag to get the text inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    date = cells[0].string\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell (`[1]`), the `.string` attribute will give you the name of the company, which is something you'd probably want. But how to get the link to the actual letter? If you inspect the HTML, the URL is attached to the `href` attribute in the `a` tag inside the `td`.\n",
    "\n",
    "The syntax to drill down to get the URL is: `cells[1].a['href']`.\n",
    "\n",
    "In human words, this means:\n",
    "- Go to the `cells` list (which contains all of the `td` tags in that `tr`) and grab the second `[1]` item: `cells[1]`\n",
    "- Grab the `a` tag inside of that `td` tag: `cells[1].a`\n",
    "- Access the `href` attribute attached to that `a` tag: `cells[1].a['href']`\n",
    "\n",
    "Which gets us this far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    date = cells[0].string\n",
    "    company = cells[1].string\n",
    "    link = cells[1].a['href']\n",
    "\n",
    "    # create a list for your data to live in\n",
    "    data_out = [date, company, link]\n",
    "    print(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing on the URL: As it's stored, it's a _relative_ link -- it doesn't include the actual domain information (`https://www.fda.gov`). It'd be a good idea to prepend this to make it a _fully qualified_ URL before we do anything to it.\n",
    "\n",
    "In Python, to concatenate two strings, just use a plus sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    date = cells[0].string\n",
    "    company = cells[1].string\n",
    "    link = 'https://www.fda.gov' + cells[1].a['href']\n",
    "\n",
    "    # create a list for your data to live in\n",
    "    data_out = [date, company, link]\n",
    "    print(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Try it yourself\n",
    "\n",
    "Now that you've gotten this far, see if you can isolate the other three pieces of data in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    date = cells[0].string\n",
    "    company = cells[1].string\n",
    "    link = 'https://www.fda.gov' + cells[1].a['href']\n",
    "\n",
    "    # issuing office\n",
    "    \n",
    "    # subject\n",
    "    \n",
    "    # close-out date\n",
    "    \n",
    "    # create a list for your data to live in\n",
    "    # add your new items to this list\n",
    "    data_out = [date, company, link]\n",
    "\n",
    "    print(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the results to file\n",
    "\n",
    "Now that we've targeted our lists of data for each row, we can use Python's built-in [`csv`](https://docs.python.org/3/library/csv.html) module to write each list to a CSV file.\n",
    "\n",
    "First, import the csv module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a list of headers to match the data (each column header will be a string) -- run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['letter_date', 'company', 'link', 'issuing_office', 'subject', 'close_out_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using something called a `with` block, open a new CSV file to write to and write some code to do the following things:\n",
    "- Create a `csv.writer` object\n",
    "- Write out the list of headers using the `writerow()` method of the `csv.writer` object\n",
    "- Drop in the `for` loop you just wrote and, instead of printing, use the `writerow()` method of the `csv.writer` object to write the list of data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file called 'fda-data.csv' in write ('w') mode\n",
    "# specify that newlines are terminated by an empty string (this deals with a PC-specific problem)\n",
    "# and use the `as` keyword to name the open file handler (the variable name is arbitrary)\n",
    "with open('fda-data.csv', 'w', newline='') as outfile:\n",
    "    # go to the csv module we imported and make a new .writer object attached to the open file\n",
    "    # and save it to a variable\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # write out the list of headers\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # paste in the for loop you wrote earlier here -- watch the indentation!\n",
    "    # it should be at this indentation level, like this\n",
    "    # for row in rows[1:]:\n",
    "    #     cells = row.find_all('td')\n",
    "    #     etc. ...\n",
    "    # but at the end, instead of `print(data_out)`\n",
    "    # make it `writer.writerow(data_out)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the folder, you should see a new file: `fda-data.csv`. Hooray!\n",
    "\n",
    "üéâ üéâ üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Try it yourself\n",
    "\n",
    "Putting it all together:\n",
    "- Find a website you'd like to scrape\n",
    "- Use `requests` to fetch the HTML\n",
    "- Use `bs4` to parse the HTML and isolate the data you're interested in\n",
    "- Use `csv` to write the data to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
